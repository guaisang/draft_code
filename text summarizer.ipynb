{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FrequencySummarizer:\n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        \"\"\"\n",
    "        Initilize the text summarizer.\n",
    "        Words that have a frequency term lower than min_cut \n",
    "        or higer than max_cut will be ignored.\n",
    "        \"\"\"\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut \n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "    def _compute_frequencies(self, word_sent):\n",
    "        \"\"\" \n",
    "        Compute the frequency of each of word.\n",
    "        Input: \n",
    "        word_sent, a list of sentences already tokenized.\n",
    "        Output: \n",
    "        freq, a dictionary where freq[w] is the frequency of w.\n",
    "        \"\"\"\n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self._stopwords:\n",
    "                    freq[word] += 1\n",
    "        # frequencies normalization and fitering\n",
    "        m = float(max(freq.values()))\n",
    "        for w in freq.keys():\n",
    "            freq[w] = freq[w]/m\n",
    "            if freq[w] >= self._max_cut or freq[w] <= self._min_cut:\n",
    "                del freq[w]\n",
    "        return freq\n",
    "\n",
    "    def summarize(self, text, n):\n",
    "        \"\"\"\n",
    "        Return a list of n sentences \n",
    "        which represent the summary of text.\n",
    "        \"\"\"\n",
    "        sents = sent_tokenize(text)\n",
    "        assert n <= len(sents)\n",
    "        word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        ranking = defaultdict(int)\n",
    "        for i,sent in enumerate(word_sent):\n",
    "            for w in sent:\n",
    "                if w in self._freq:\n",
    "                    ranking[i] += self._freq[w]\n",
    "        sents_idx = self._rank(ranking, n)    \n",
    "        return [sents[j] for j in sents_idx]\n",
    "\n",
    "    def _rank(self, ranking, n):\n",
    "        \"\"\" return the first n sentences with highest ranking \"\"\"\n",
    "        return nlargest(n, ranking, key=ranking.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_only_text(url):\n",
    "    \"\"\" \n",
    "    return the title and the text of the article\n",
    "    at the specified url\n",
    "    \"\"\"\n",
    "    page = urllib2.urlopen(url).read().decode('utf8')\n",
    "    soup = BeautifulSoup(page)\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    return soup.title.text, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guaisang/anaconda/lib/python2.7/site-packages/beautifulsoup4-4.4.1-py2.7.egg/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "Hundreds report football abuse to police - BBC News\n",
      "* The green green grass of one of Europe's poorest countries Why 1980s myths about HIV still endure Life in the remote community of Galena in Alaska How Rospa's poster campaigns reveal a changing nation The alpacas protecting 24,000 Christmas turkeys The boy putting female puberty on hold Business holdings that could cause dilemmas for Trump The people desperate for Chinese government jobs Who is in the running for this year's festive number one?\n",
      "*  Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp Linkedin Copy this link About 350 victims have reported child sexual abuse within UK football clubs, police chiefs have said.\n",
      "----------------------------------\n",
      "Harlow Polish man death: Teenager charged with manslaughter - BBC News\n",
      "* The green green grass of one of Europe's poorest countries Why 1980s myths about HIV still endure Life in the remote community of Galena in Alaska How Rospa's poster campaigns reveal a changing nation The alpacas protecting 24,000 Christmas turkeys The boy putting female puberty on hold Business holdings that could cause dilemmas for Trump The people desperate for Chinese government jobs Who is in the running for this year's festive number one?\n",
      "*  Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp Linkedin Copy this link A 15-year-old boy has been charged with manslaughter over the death of a Polish man who was attacked in Essex.\n",
      "----------------------------------\n",
      "Net migration stays near record high - BBC News\n",
      "* Alp Mehmet from Migration Watch UK, which calls for a reduction in levels of immigration, said the figures show Britain needs to take a \"strong position on immigration in the Brexit negotiations\".\n",
      "* He added: \"Even if net migration was brought down to 265,000 a year the UK population would still be growing at half a million a year, every year for the next ten years.\n",
      "----------------------------------\n",
      "David Davis: UK may pay for access to EU single market - BBC News\n",
      "* During his regular session facing MPs in the Commons, Mr Davis was asked a number of questions about the shape of the future Brexit deal, including one from Labour MP Wayne David, who said: \"Will the government consider making any contribution in any shape or form for access to the single market?\"\n",
      "*  Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp Linkedin Copy this link The UK would consider making payments to the EU after it leaves the bloc to secure the best possible access to the EU single market, Brexit Secretary David Davis has said.\n",
      "----------------------------------\n",
      "Councils urged to redesign speed bumps to cut pollution - BBC News\n",
      "* The green green grass of one of Europe's poorest countries Why 1980s myths about HIV still endure Life in the remote community of Galena in Alaska How Rospa's poster campaigns reveal a changing nation The alpacas protecting 24,000 Christmas turkeys The boy putting female puberty on hold Business holdings that could cause dilemmas for Trump The people desperate for Chinese government jobs Who is in the running for this year's festive number one?\n",
      "*  Share this with Email Facebook Messenger Messenger Twitter Pinterest WhatsApp Linkedin Copy this link Local authorities should consider lower speed limits, clean air zones and even redesign speed bumps in a bid to reduce air pollution, health experts say.\n"
     ]
    }
   ],
   "source": [
    "feed_xml = urllib2.urlopen('http://feeds.bbci.co.uk/news/rss.xml').read()\n",
    "feed = BeautifulSoup(feed_xml.decode('utf8'))\n",
    "to_summarize = map(lambda p: p.text, feed.find_all('guid'))\n",
    "\n",
    "fs = FrequencySummarizer()\n",
    "for article_url in to_summarize[:5]:\n",
    "    title, text = get_only_text(article_url)\n",
    "    print '----------------------------------'\n",
    "    print title\n",
    "    for s in fs.summarize(text, 2):\n",
    "        print '*',s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
